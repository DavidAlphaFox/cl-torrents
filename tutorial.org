
* Step 1 - first start

Scraping   The   Pirate   Bay   is    easy,   they   don't   rely   on
javascript-generated pages. We just have to:

- get the html page ([[http://quickdocs.org/dexador/][dexador]]: =(dex:get <url>)=)
- parse the html into a data structure ([[https://shinmera.github.io/plump/][plump]]: =(plump:parse <html>)=)
- search  this  request  with   CSS  selectors  ([[https://shinmera.github.io/lquery][lquery]]:  =(lquery:"
  <parsed-html> <selectors>)=)

Let's go.

To begin  with, we do  a search on the  website and we  copy-paste the
url. We get one like this:

#+BEGIN_SRC lisp
(defparameter *search-url* "https://piratebay.to/search/?FilterStr=matrix&ID=&Limit=800&Letter=&Sorting=DSeeder" "base search url. KEYWORDS to be replaced.")
#+END_SRC

it has our search term in it (=matrix=) along with url parameters.

We will use CSS selectors to extract information from the web page, so
we can use  our browser's developer tools to inspect  the structure of
the page  and guess our  selectors: right  click on a  result's title,
choose "inspect element". It highlights some html similar to this:

#+BEGIN_SRC html
  <td class="Title">
    <span class="ColorA">
      <a href="https://piratebay.to/torrent/1922147/Matrix (1999)Blu-Ray 720p Dublado PT-BR - mo93438/" onclick="Javascript:OpenDetailPage('https://piratebay.to/torrent/1922147/Matrix (1999)Blu-Ray 720p Dublado PT-BR - mo93438/'); return false;">Matrix (1999)Blu-Ray 720p Dublado PT-BR - mo93438 </a>
    </span>
    <br>
      <span class="ColorB VaA">Upload Date: 20.02.15 </span>
      <span class="ColorB VaA">Size: 796,86 MB </span>
      <span class="ColorB"> </span>
  </td>
#+END_SRC


The title is  well delimited so we'll start selecting  our elements by
the CSS class =Title=, which gives:

#+BEGIN_SRC lisp
(defparameter *selectors* ".Title")
#+END_SRC

Install our dependencies:

#+BEGIN_EXPORT latex
(ql:quickload '("dexador" "plump" "lquery"))
#+END_EXPORT


** Trying out at the REPL

Let's try in the REPL:

#+BEGIN_SRC lisp
(defparameter html (dex:get *search-url*)) ;; returns the html
(defparameter parsed (plump:parse html))   ;; returns a list of plump elements
(defparameter results (lquery:$ parsed *selectors*)) ;; returns a list of stuff
(setf results (lquery:$ parsed *selectors* (text)))  ;; returns text, i.e. the titles
#+END_SRC

A little explanation for the =lquery=  line: the last =(text)= part is
an lquery thing to get the text representation of the node, instead of
a lquery internal object.


I  like to  check the  html content  of the  plump nodes.  We use  the
=serialize= plump function (second function from its doc ;) ):

#+BEGIN_SRC lisp
CL-TORRENTS> (plump:serialize (first res))
<th class="Title header ">
<a href="https://piratebay.to/search/0/800/0/matrix/0/ATitle/1/">Title</a>
</th>
">"
#+END_SRC

err… not much in  it. Will have to check what is  the first element of
the list.

#+BEGIN_SRC lisp
  CL-TORRENTS> (plump:serialize (second res))
  <td class="Title">
  <span class="ColorA">
  <a href="https://piratebay.to/torrent/2297350/Matrix FRENCH DVDRIP 1999 COOL/"
     onclick="Javascript:OpenDetailPage('https://piratebay.to/torrent/2297350/Matrix FRENCH DVDRIP 1999 COOL/'); return false;">Matrix FRENCH DVDRIP 1999 COOL
  </a>
  </span>
  <br/>
  <span class="ColorB VaA">Upload Date: 05.06.15
  </span>
  <span class="ColorB VaA">Size: 700,30 MB
  </span>
  <span class="ColorB"/>
  </td>
  ">"
#+END_SRC

allright, good, just  checking. It has the link to  the torrent's page
inside the href of the "a" element, just as usual. We'll get to it.

Note that we  can also inspect the results with  the mouse: left/right
clicks  on the  elements printed  in the  REPL get  us into  the Slime
inspector.

** Putting it together in a function

We came up with this function:

#+BEGIN_SRC lisp
(defun torrents (words)
  ""
  (let* ((html (dex:get *search-url*))
         (parsed (plump:parse html))
         (res (lquery:$ parsed *selectors* (text))))
    res))
#+END_SRC

and if  you prefer a  threading macro /  pipes, no problem,  but we'll
load another external library:

# TODO: arrow-macros has more cool stuff

#+BEGIN_EXPORT latex
(ql:quickload "cl-arrows")
;; and we import its symbols:
(use-package "cl-arrows")
#+END_EXPORT

#+BEGIN_SRC lisp
(-<>> *search-url*
  (dex:get)
  (plump:parse)
  (lquery:$ <> *selectors* (text)))
#+END_SRC

[[https://github.com/nightfly19/cl-arrows][cl-arrows]] defines  a few  threading macros. The  classic one  would be
=->=,  which inserts  the  result  of the  preceding  form as  _first_
argument, =->>= that  puts it _last_, which is what  we wanted for the
two  forms but  not for  the last  one, with  lquery, which  needs the
parsed  html as  first argument.   So we  use =-<>>=:  the arrow  will
populate  the  _last_  argument,  except when  it  encounters  a  =<>=
placeholder. =-<>>= has a little name, "Diamond Spear".

** Creating a new project

 We use =cl-project=:

 #+BEGIN_SRC lisp
 (ql:quickload "cl-project")
 (cl-project:make-project #P"~/path/to/cl-torrents/")
 #+END_SRC

 Note that it  may be easier for  you sometimes if you  create your new
 Common  Lisp  projects  into =~/.quicklisp/local-projects=  (known  by
 Quicklisp) or =~/.local/share/common-lisp/= (known by ASDF).

** Loading the project

Open the  =.asdf= file at  the project root  and compile and  load the
file. In  Slime, it's  with =C-c  C-k= (=slime-compile-and-load-file=,
see also the Emacs menu).

Now we can load the project at the REPL:

#+BEGIN_SRC lisp
(asdf:make "cl-torrents"
; compiling file "/home/vince/projets/cl-torrents/src/cl-torrents.lisp" (written 28 AUG 2017 10:21:07 PM):
; compiling (IN-PACKAGE :CL-USER)
; compiling (DEFPACKAGE CL-TORRENTS ...)
; compiling (IN-PACKAGE :CL-TORRENTS)
; compiling (DEFPARAMETER *SEARCH-URL* ...)
; compiling (DEFPARAMETER *SELECTORS* ...)
; compiling (DEFUN TORRENTS ...)

; /home/vince/.cache/common-lisp/sbcl-1.3.19-linux-x64/home/vince/projets/cl-torrents/src/cl-torrents-tmp5GEXGEG5.fasl written
; compilation finished in 0:00:00.029
; compilation unit finished
T
#+END_SRC

And now we can use our function with the project prefix:

#+BEGIN_SRC lisp
(cl-torrents:torrents "matrix")
#+END_SRC

or import our package and call our functions directly:

#+BEGIN_SRC lisp
(in-package :cl-torrents)
#+END_SRC

** Adding our dependencies

Our new =cl-torrents.asd= looks like this:

#+BEGIN_SRC lisp
#|
  This file is a part of cl-torrents project.
|#

(in-package :cl-user)
(defpackage cl-torrents-asd
  (:use :cl :asdf))
(in-package :cl-torrents-asd)

(defsystem cl-torrents
  :version "0.1"
  :author ""
  :license ""
  :depends-on ()  ;; <== list of dependencies
  :components ((:module "src"
                :components
                ((:file "cl-torrents"))))
  :description ""
  :long-description
  …)
#+END_SRC

For pythonistas, it is very similar to a =setup.py=.

It  has the  =depends-on= paramater  which accepts  a list  of package
names. We have to register here =dexador= and the others:

#+BEGIN_SRC lisp
  :depends-on (:str
               :dexador
               :plump
               :lquery)
#+END_SRC

** Searching with our keywords

Until now  we only tried  things out with a  given search url,  set in
stone. It's time to insert our own search terms into this search url.

We'll put a ={KEYWORDS}= placeholder into the url:

#+BEGIN_SRC lisp
(defparameter *search-url* "https://piratebay.to/search/?FilterStr={KEYWORDS}&ID=&Limit=800&Letter=&Sorting=DSeeder" "base search url. KEYWORDS to be replaced.")
#+END_SRC

which we will replace with a =+=-separated list of keywords.

With a little  look at the [[https://lispcookbook.github.io/cl-cookbook/strings.html]["strings" cookbook page]],  we'll go with the
little [[https://github.com/vindarel/cl-str][str]] library (our lib actually):

#+BEGIN_SRC lisp
(ql:quickload "str")
#+END_SRC

Let's try:

#+BEGIN_SRC lisp
(defparameter words "matrix trilogy")
;; => WORDS
(str:words words)
;; => ("matrix" "trilogy")
(str:join "+" *) ;; the * is a REPL shortcut to insert the previous result. + inserts the previous input.
;; => "matrix+trilogy"
#+END_SRC

and voilà. We put this at the beginning of our search function and we get:

#+BEGIN_SRC lisp
(defparameter *search-url* "https://piratebay.to/search/?FilterStr={KEYWORDS}&ID=&Limit=800&Letter=&Sorting=DSeeder" "base search url. KEYWORDS to be replaced.")

(defun torrents (words)
  "Search torrents."
  (let* ((terms (str:words words))
         (query (str:join "+" terms))
         (*search-url* (str:replace-all "{KEYWORDS}" query *search-url*))
         (req (dex:get *search-url*))
         (html (plump:parse req))
         (res (lquery:$ html *selectors* (text))))
    res))
#+END_SRC

Let's try:

#+BEGIN_SRC lisp
(torrents "matrix trilogy")
#("
Title
"
  "Matrix FRENCH DVDRIP 1999 COOLUpload Date: 05.06.15 Size: 700,30 MB"
  "The Matrix Reloaded (2003) FullHD, Dual Audio: English + SpaUpload Date: 12.04.15 Size: 8,51 GB"
  "The Matrix Trilogy (1999-2003) + Extras 1080p BluRay x264 DuUpload Date: 12.02.15 Size: 12,86 GB"
  "The Matrix Trilogy (1999-2003) BluRay BDRip 1080p AC3Upload Date: 15.09.15 Size: 23,29 GB"
  "The Matrix Trilogy (1999-2003) BDRip 1080p Dual Audio [ HindUpload Date: 14.01.15 Size: 10,23 GB"
  "The Matrix Revolutions (2003) BRRip [Dual Audio] [Hindi+Eng]Upload Date: 24.02.15 Size: 496,36 MB"
  "Matrix (1999)Blu-Ray 720p Dublado PT-BR - mo93438Upload Date: 20.02.15 Size: 796,86 MB"
  "The Matrix Reloaded (2003) BRRip [Dual Audio] [Hindi+Eng] 50Upload Date: 22.02.15 Size: 496,39 MB"
  [and more results]
#+END_SRC

Cool !

We can commit  this, have a break  and enjoy how things  are going. It
was very easy !

Of course, we need  to get more stuff out of  this, like the torrent's
magnet link.

** Getting more torrent information

With =plump:serialize=  we could check  what html is inside  our plump
node:

#+BEGIN_SRC lisp
  (plump:serialize (second res))
  <td class="Title">
  <span class="ColorA">
  <a href="https://piratebay.to/torrent/2297350/Matrix FRENCH DVDRIP 1999 COOL/" onclick="Javascript:OpenDetailPage('https://piratebay.to/torrent/2297350/Matrix FRENCH DVDRIP 1999 COOL/'); return false;">Matrix FRENCH DVDRIP 1999 COOL
  </a>
  </span>
  <br/>
  <span class="ColorB VaA">Upload Date: 05.06.15
  </span>
  <span class="ColorB VaA">Size: 700,30 MB
  </span>
  <span class="ColorB"/>
  </td>
#+END_SRC

and we want to get the href with the torrent's url.

We know how to access the =a=:

#+BEGIN_SRC lisp
(defparameter *elt* (second res))
(lquery:$ *elt* "a" (text))
;; => #("Matrix FRENCH DVDRIP 1999 COOL")
#+END_SRC

it returns a plump node.

Let's try getting the href:

#+BEGIN_SRC lisp
(lquery:$ *elt* "a[href]" (text))
;; => #("Matrix FRENCH DVDRIP 1999 COOL")
#+END_SRC

this still returns the plump node.

With  the REPL  autocompletion and  then a  check at  the doc  we find
=lquery-funcs:attr= to extract attributes:

#+BEGIN_SRC lisp
(lquery-funcs:attr (lquery:$ *elt* "a") "href")
;; => #("https://piratebay.to/torrent/2297350/Matrix FRENCH DVDRIP 1999 COOL/")
#+END_SRC

Ok.

But watch out,  the =#(…)= denotes a vector. We  get its first element
with

#+BEGIN_SRC lisp
(aref * 0)
#+END_SRC

(see the [[https://lispcookbook.github.io/cl-cookbook/data-structures.html][Cookbook's Data Structures page]]).

We put this in a function:

#+BEGIN_SRC lisp
(defun detail-page-url (node)
  "Extract the link of the details page. `node': plump node, containing the url."
  (let* ((href-vector (lquery-funcs:attr (lquery:$ node "a") "href"))
         (href (aref href-vector 0)))
    href))
#+END_SRC

which we can test (either write it  at the REPL either write it in the
project and compile, =C-c C-c= in Slime):

#+BEGIN_SRC lisp
(mapcar #'detail-page-url res)  ;; #' shorthand for function
;; =>
("https://piratebay.to/search/0/800/0/matrix/0/ATitle/1/"
 "https://piratebay.to/torrent/2297350/Matrix FRENCH DVDRIP 1999 COOL/"
 "https://piratebay.to/torrent/2156107/The Matrix Reloaded (2003) FullHD, Dual Audio: English + Spa/"
 "https://piratebay.to/torrent/1885366/The Matrix Trilogy (1999-2003) + Extras 1080p BluRay x264 Du/"
[…]
#+END_SRC

*** To the magnet link

We have the torrent's details page, we  know how to request it, now we
want to get the magnet link.

#+BEGIN_SRC lisp
(mapcar (lambda (it)
          (lquery-funcs:attr it "href"))
        (coerce (lquery:$ * "a") 'list))
;; =>
(NIL NIL NIL NIL NIL NIL NIL "https://piratebay.to/" "https://piratebay.to/"
 […]
 "http://imdb.com/title/tt1778413/" "https://piratebay.to/profile/Anonym"
 "https://piratebay.to/Downloader.php?ID=2289391&Filename=Matrix+FRENCH+DVDRIP+1999+COOL"
 "magnet:?xt=urn:btih:40eca43690cf1b99b0a4d485ebf4855d20b0bac5" "http://"
 […]
 "https://twitter.com/piratebayto" "https://www.facebook.com/thepiratebayto"
 "http://bitcoin.org" "bitcoin:1FX2wz8NiWPdtCGSrzn7j3NAg4VKqGovto" "/")
#+END_SRC

We  saw  what  I  consider  CL oddities  and  quirks  here,  for  sure
frustrating stuff  for the  (impatient) beginner.  =mapcar=  expects a
list and  lquery returns a vector,  so we had to  transform the result
with =coerce=.  Also  the name =mapcar= is a bit  outated, even if the
other map functions in the family  can be useful.  There is =map= that
takes its output type has first  parameter: =(map 'list …)=. There are
fixes.

We could  use [[https://lispcookbook.github.io/cl-cookbook/cl21.html][cl21]]'s =map=, which works on  vectors too.

Still with cl21, we can write shorter lambdas, with the shorthand =lm=
or with:

#+BEGIN_SRC lisp
(map ^(lquery-funcs:attr % "href") …) ;; with more arguments, use %1, %2,…
#+END_SRC

Cl21 defines and  rewrites a lot of stuff to  offer shorter and sanier
ways of doing (this,  hash-tables, regexps,…), towards more functional
programming, and more generics (functions that work on many types, not
one function  per type). It still  is a CL  library, so we can  use it
alongside the usual CL in our project.

It is written  by a super productive and innovative  CL hacker and has
600+ stars  on github.  Nevertheless, it wasn't  touched in  two years
and, as it lacks docstrings and  direction, we can be surprised by the
new implementation of some functions. No  need to say the community is
divided on this subject.

Filtering:

#+BEGIN_SRC lisp
(remove-if-not (lambda (it)
                 (str:starts-with? "magnet" it))
               *)
#+END_SRC

Here, I  used again a short  verb from an external  library for string
manipulation. The CL way would be something like:

#+BEGIN_SRC lisp
(string= "magnet-foo" "magnet" :start1 0 :end1 (length "magnet"))
T
#+END_SRC

and yet we must handle nils, differences of length,… so boring.

** Conclusion

This leads us to the end of part one. It was very easy !

We now want or need more:

- getting more content: the magnet links, each torrent's page
- downloading the torrent files ?
- error handling (network errors, unexpected errors)
- scraping other sites, asynchronously  (the asynchronous part will be
  straightforward, there's  a library for  that and it's  one function
  change)
- some tests
- some cache
- a command line tool
